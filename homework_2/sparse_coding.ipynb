{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65ae6477",
   "metadata": {},
   "source": [
    "# Sparse Coding\n",
    "\n",
    "Author(s): Raj Magesh Gauthaman (rgautha1@jh.edu)\n",
    "\n",
    "If you find any bugs in this notebook, please email Raj (rgautha1@jh.edu)!\n",
    "\n",
    "---\n",
    "\n",
    "Our goal is to obtain a dictionary of basis vectors that form a sparse encoding of natural images. Note that we're using the term \"basis\" loosely: the [definition of a basis](https://en.wikipedia.org/wiki/Basis_(linear_algebra)) requires its vectors to be linearly independent and span the entire space; here, we impose no such constraints.\n",
    "\n",
    "We randomly sample patches $\\{P_n\\}_{n \\in \\mathbb{Z}}$ from the [ten $512 \\times 512$ natural images](http://www.rctn.org/bruno/sparsenet/) used in [Olshausen & Field (1996)](https://www.nature.com/articles/381607a0). Note that the images are standardized, whitened, and low-pass filtered (similar to the procedure described by Olshausen & Field) and that each patch is independently standardized. Each patch has size $p \\times p$, making each basis vector $p^2$-dimensional. If $m > p^2$, the basis is *overcomplete*. Our $m$ $p^2$-dimensional basis vectors form a dictionary $X \\in \\mathbb{R}^{p^2 \\times m}$. Our goal is to represent each flattened image patch $P \\in \\mathbb{R}^{p^2}$ as a linear combination of these basis vectors (i.e., $P \\approx XZ$) with sparse coefficients $Z \\in \\mathbb{R}^{m}$. These considerations inform the loss function we'd like to minimize:\n",
    "\n",
    "$$L(X, Z) = \\frac{1}{2} \\lVert P - XZ \\rVert^2_2 + \\lambda |Z|$$\n",
    "\n",
    "Here, the first term is the $L_2$-norm of the reconstruction error induced when the original image patch $P$ is reconstructed as $\\hat{P} = XZ$. The second term is the $L_1$-norm of the coefficients $Z$. The regularization parameter $\\lambda$ controls the tradeoff between sparsity and the reconstruction error. Minimizing $L$ with respect to $Z$ would produce a sparse representation $Z$. We implement this optimization using the coordinate descent algorithm described in [Gregor & LeCun (2010)](https://icml.cc/Conferences/2010/papers/449.pdf) and introduced by [Li & Osher (2009)](https://doi.org/10.3934/ipi.2009.3.487). However, our goal is to learn the dictionary $X$, which involves minimizing $L$ with respect to $X$. We can derive this gradient analytically:\n",
    "\n",
    "$$\\nabla_X L(X, Z) = -(P - XZ)Z^T$$\n",
    "\n",
    "We optimize $X$ iteratively using stochastic gradient descent:\n",
    "\n",
    "1. We initialize $X_0$ with values drawn from a uniform distribution on $[-0.5, 0.5)$.\n",
    "2. At each iteration $i$, we\n",
    "   1. sample a batch of image patches $P_j$, $j \\in [b]$, where $b$ is the batch size,\n",
    "   2. use coordinate descent to find the optimal sparse representation $Z_j$ for each image patch $P_j$ in terms of the basis vectors $X_i$,\n",
    "   3. compute the gradient of the loss for each patch as $\\nabla_X L_j(X, Z_j) = -(P_j - X_iZ_j)Z_j^T$,\n",
    "   4. update $X$ as $X_{i+1} = X_i - \\dfrac{\\eta}{b} \\sum_j \\nabla_X L_j(X, Z_j) = X_i + \\dfrac{\\eta}{b} \\sum_j (P_j - X_iZ_j)Z_j^T$, where $\\eta$ is the learning rate, and\n",
    "   5. normalize the basis vectors (columns) in  $X_i$ to unit length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ed864",
   "metadata": {},
   "source": [
    "Note: Uncomment and run the following cell if you are using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4216e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51467c36",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbfa48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections.abc import Collection, Iterable\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import math\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as tr\n",
    "from torchdata.datapipes.iter import IterDataPipe, Mapper, IterableWrapper, Shuffler\n",
    "from scipy.io import loadmat\n",
    "from scipy.sparse.linalg import cg\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def download(\n",
    "    url: str,\n",
    "    *,\n",
    "    filepath: Path = None,\n",
    "    stream: bool = True,\n",
    "    allow_redirects: bool = True,\n",
    "    chunk_size: int = 1024**2,\n",
    "    force: bool = True,\n",
    ") -> Path:\n",
    "    \"\"\"Download a file from a URL.\"\"\"\n",
    "\n",
    "    if filepath is None:\n",
    "        filepath = Path(\"/tmp\") / f\"{uuid.uuid4()}\"\n",
    "    elif filepath.exists():\n",
    "        if not force:\n",
    "            return filepath\n",
    "        else:\n",
    "            filepath.unlink()\n",
    "\n",
    "    r = requests.Session().get(url, stream=stream, allow_redirects=allow_redirects)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            f.write(chunk)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def download_images(\n",
    "    url: str = \"http://www.rctn.org/bruno/sparsenet/IMAGES_RAW.mat\",\n",
    "    directory: Path = Path(\"data/sparse_coding/images\"),\n",
    ") -> list[Image.Image]:\n",
    "    \"\"\"Download the images used in Olshausen & Field (1996).\"\"\"\n",
    "\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    filepath = download(url, filepath=directory / \"images.mat\", force=False)\n",
    "    images = loadmat(filepath, simplify_cells=True)[\"IMAGESr\"]\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    for i_image in range(10):\n",
    "        image = images[..., i_image]\n",
    "        min_ = image.min()\n",
    "        max_ = image.max()\n",
    "        image = (image - min_) / (max_ - min_) * 255\n",
    "        image = Image.fromarray(image).convert(\"L\")\n",
    "\n",
    "        path = directory / f\"image_{i_image:02}.png\"\n",
    "        paths.append(path)\n",
    "        image.save(path, format=\"PNG\")\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_images(paths: Collection[Path]) -> list[Image.Image]:\n",
    "    \"\"\"Load images from disk.\"\"\"\n",
    "    return [Image.open(path) for path in paths]\n",
    "\n",
    "\n",
    "def display_images(images: Collection[Image.Image | torch.Tensor]) -> None:\n",
    "    \"\"\"Display images\"\"\"\n",
    "    n = len(images)\n",
    "    nrows = int(math.sqrt(n))\n",
    "    ncols = math.ceil(n / nrows)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    for image, ax in zip(images, axes.flat):\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.cpu().numpy()\n",
    "        ax.imshow(image, cmap=\"gray\")\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def coordinate_descent(\n",
    "    *,\n",
    "    w: torch.Tensor,\n",
    "    x: torch.Tensor,\n",
    "    alpha: float,\n",
    "    max_iterations: int = 1000,\n",
    "    tolerance: float = 1e-4,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Coordinate descent algorithm described in Gregor & LeCun (2010).\n",
    "\n",
    "    Minimizes L(z) = 1/2 (x - wz)^2 + alpha * |z| with respect to z. Supports batches of data x.\n",
    "\n",
    "    References:\n",
    "\n",
    "    * Gregor & LeCun (2010), https://icml.cc/Conferences/2010/papers/449.pdf, Algorithm 2, Coordinate Descent [Pg 3]\n",
    "    * Li & Osher (2009), https://doi.org/10.3934/ipi.2009.3.487, Algorithm (Coordinate descent with a refined sweep) [Pg 6]\n",
    "\n",
    "    Args:\n",
    "        w: (n, m) dictionary of m n-dimensional basis vectors\n",
    "        x: n-dimensional data or (n, batch_size) batch of data\n",
    "        alpha: Regularization strength, controls sparsity of the solution z\n",
    "        max_iterations: Maximum number of iterations. Defaults to 1000.\n",
    "        tolerance: Tolerance for early stopping. Defaults to 1e-4.\n",
    "\n",
    "    Returns:\n",
    "        z: (m, batch_size) coefficients of the data x under the basis w\n",
    "    \"\"\"\n",
    "    device = w.device\n",
    "    assert w.ndim == 2, \"w must be 2-dimensional\"\n",
    "    x = x.unsqueeze(-1) if x.ndim == 1 else x\n",
    "\n",
    "    s = torch.eye(w.shape[-1], device=device) - w.t() @ w\n",
    "    b = w.t() @ x\n",
    "    z = torch.zeros_like(b, device=device)\n",
    "\n",
    "    slicer = torch.arange(x.shape[-1])\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        z_shrunk = torch.nn.functional.softshrink(b, lambd=alpha)\n",
    "        diff = z_shrunk - z\n",
    "        diff_abs = diff.abs()\n",
    "        k = diff_abs.argmax(dim=0)\n",
    "        b += s[..., k] * diff[k, slicer]\n",
    "        z[k, slicer] = z_shrunk[k, slicer]\n",
    "\n",
    "        if diff_abs[k, slicer].max() < tolerance:\n",
    "            break\n",
    "\n",
    "    z = torch.nn.functional.softshrink(b, lambd=alpha)\n",
    "    return z\n",
    "\n",
    "\n",
    "def create_sinusoid(\n",
    "    *,\n",
    "    size: tuple[int, int] = (32, 32),\n",
    "    amplitude: float = 1,\n",
    "    frequency: float = 0.1,\n",
    "    angle: float = 0,\n",
    "    phase: float = 0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Creates a 2D sinusoid.\n",
    "\n",
    "    Args:\n",
    "        size: Size of the sinusoid, defaults to (32, 32)\n",
    "        amplitude: Amplitude of the sinusoid, defaults to 1\n",
    "        frequency: Frequency of the sinusoid, defaults to 0.1\n",
    "        angle: Angle of the sinusoid, defaults to 0 (horizontal)\n",
    "        phase: Phase of the sinusoid, defaults to 0 (sine)\n",
    "\n",
    "    Returns:\n",
    "        2D sinusoid\n",
    "    \"\"\"\n",
    "    angle = torch.tensor(angle)\n",
    "    w_x, w_y = frequency * torch.cos(angle), frequency * torch.sin(angle)\n",
    "    width, height = size\n",
    "    radius = (width // 2, height // 2)\n",
    "    [x, y] = torch.meshgrid(\n",
    "        torch.arange(-radius[0], radius[0] + 1),\n",
    "        torch.arange(-radius[1], radius[1] + 1),\n",
    "        indexing=\"ij\",\n",
    "    )\n",
    "\n",
    "    return amplitude * torch.cos(w_x * x + w_y * y + phase)\n",
    "\n",
    "\n",
    "def create_sinusoids(\n",
    "    *,\n",
    "    frequency: float = 0.1,\n",
    "    size: tuple[int, int] = (32, 32),\n",
    "    n: int = 10,\n",
    ") -> list[torch.Tensor]:\n",
    "    \"\"\"Sample sinusoids of a given frequency.\n",
    "\n",
    "    The amplitudes, phases, and angles of the sinuoids are sampled from [0, 1), [0, 2 * pi), and [0, pi) respectively.\n",
    "\n",
    "    Args:\n",
    "        frequency: Frequency of the sinusoids. Defaults to 0.1.\n",
    "        size: Size of the sinusoids. Defaults to (32, 32).\n",
    "        n: Number of sinuoids to create. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        Sinusoids\n",
    "    \"\"\"\n",
    "    sinusoids = []\n",
    "    for _ in range(n):\n",
    "        amplitude = random.random()\n",
    "        phase = 2 * math.pi * random.random()\n",
    "        angle = math.pi * random.random()\n",
    "        sinusoids.append(\n",
    "            create_sinusoid(\n",
    "                size=size,\n",
    "                amplitude=amplitude,\n",
    "                frequency=frequency,\n",
    "                angle=angle,\n",
    "                phase=phase,\n",
    "            )\n",
    "        )\n",
    "    return sinusoids\n",
    "\n",
    "\n",
    "def extract_patches_from_images(\n",
    "    images: Iterable[Image.Image | torch.Tensor],\n",
    "    *,\n",
    "    patch_size: int = 8,\n",
    "    n_patches_per_image: int = 10,\n",
    "    n_images_per_batch: int = 10,\n",
    ") -> IterDataPipe:\n",
    "    \"\"\"Extract square patches from a stream of images.\n",
    "\n",
    "    Images are standardized and preprocessed (whitened and low-pass filtered) according to Olshausen & Field (1996). Square patches are extracted from each image and individually standardized.\n",
    "\n",
    "    References:\n",
    "\n",
    "        * Olshausen & Field (1996), https://doi.org/10.1038/381607a0\n",
    "        * Olshausen & Field (1997), https://doi.org/10.1016/S0042-6989(97)00169-7\n",
    "\n",
    "    Args:\n",
    "        images: Images to extract patches from\n",
    "        patch_size: Size of each square patch. Defaults to 8.\n",
    "        n_patches_per_image: Number of patches to extract from each image in the batch. Defaults to 10.\n",
    "        n_images_per_batch: Number of images to use in each batch. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        Datapipe that returns batches of patches of shape (patch_size ** 2, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def _preprocess_image(image: torch.Tensor) -> torch.Tensor:\n",
    "        assert image.shape[-1] == image.shape[-2], \"image must be square\"\n",
    "        size = image.shape[-1]\n",
    "        f0 = 0.4 * size\n",
    "        f1 = torch.fft.fftfreq(size) * size\n",
    "        f2 = torch.fft.rfftfreq(size) * size\n",
    "        f1, f2 = torch.meshgrid(f1, f2, indexing=\"ij\")\n",
    "        r = torch.sqrt(f1**2 + f2**2)\n",
    "        filter_ = r * torch.exp(-((r / f0) ** 4))\n",
    "        return torch.fft.irfft2(filter_ * torch.fft.rfft2(image))\n",
    "\n",
    "    def _collate_fn(\n",
    "        images: Collection[Image.Image | torch.Tensor],\n",
    "        n_patches_per_image: int = n_patches_per_image,\n",
    "    ) -> torch.Tensor:\n",
    "        transform = tr.Compose(\n",
    "            [\n",
    "                tr.ToTensor(),\n",
    "                tr.Normalize(mean=(0,), std=(1,)),\n",
    "            ]\n",
    "        )\n",
    "        crop = tr.RandomCrop(size=patch_size)\n",
    "\n",
    "        patches = []\n",
    "        for image in images:\n",
    "            if isinstance(image, Image.Image):\n",
    "                image = transform(image)\n",
    "            elif isinstance(image, torch.Tensor):\n",
    "                image = (image - image.mean()) / image.std()\n",
    "            image = _preprocess_image(image)\n",
    "            patches.extend([crop(image).flatten() for _ in range(n_patches_per_image)])\n",
    "        patches = torch.stack(patches, dim=-1)\n",
    "        return (patches - patches.mean(dim=0, keepdim=True)) / patches.std(\n",
    "            dim=0, keepdim=True\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        IterableWrapper(images)\n",
    "        .cycle()\n",
    "        .shuffle(buffer_size=n_images_per_batch)\n",
    "        .batch(batch_size=n_images_per_batch)\n",
    "        .collate(_collate_fn)\n",
    "        .in_memory_cache()\n",
    "    )\n",
    "\n",
    "\n",
    "def learn_sparse_encoding(\n",
    "    datapipe: IterDataPipe,\n",
    "    *,\n",
    "    regularization_strength: float = 0.5,\n",
    "    dictionary: torch.Tensor = None,\n",
    "    dictionary_size: int = 64,\n",
    "    learning_rate: float = 0.1,\n",
    "    n_batches: int = 100,\n",
    "    device: torch.device | str | None = None,\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Learn a dictionary of basis functions for sparse encoding for images.\n",
    "\n",
    "    Args:\n",
    "        datapipe: Datapipe that returns batches of images of shape (n_pixels, batch_size)\n",
    "        regularization_strength: Parameter that controls the sparsity of the encoding. Defaults to 0.5.\n",
    "        dictionary: (Optional) Dictionary to initialize the optimization with (hot-start). Defaults to None.\n",
    "        dictionary_size: Number of basis vectors to include in the dictionary. Ignored if dictionary is provided. Defaults to 64.\n",
    "        learning_rate: Learning rate for stochastic gradient descent. Defaults to 0.1.\n",
    "        n_batches: Number of batches to sample from the datapipe. Defaults to 100.\n",
    "        device: PyTorch device (CPU/GPU) to run the optimization on\n",
    "        **kwargs: Passed to coordinate_descent().\n",
    "\n",
    "    Returns:\n",
    "        (n_pixels, dictionary_size) dictionary containing basis vectors\n",
    "    \"\"\"\n",
    "    if device is not None:\n",
    "        device = torch.device(device)\n",
    "    else:\n",
    "        device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "    for batch, patches in tqdm(\n",
    "        zip(range(n_batches), datapipe), desc=\"iteration\", leave=False, total=n_batches\n",
    "    ):\n",
    "        if dictionary is not None:\n",
    "            assert (\n",
    "                dictionary.shape[0] == patches.shape[0]\n",
    "            ), f\"dictionary must have {patches.shape[0]}-dimensional basis vectors\"\n",
    "            dictionary = dictionary.to(device)\n",
    "        else:\n",
    "            dictionary = (\n",
    "                torch.rand(size=(patches.shape[0], dictionary_size), device=device)\n",
    "                - 0.5\n",
    "            )\n",
    "\n",
    "        dictionary /= dictionary.norm(dim=0, keepdim=True)\n",
    "        patches = patches.to(device)\n",
    "        coefficients = coordinate_descent(\n",
    "            w=dictionary, x=patches, alpha=regularization_strength, **kwargs\n",
    "        )\n",
    "        dictionary += learning_rate * torch.einsum(\n",
    "            \"bi,bj->bij\",\n",
    "            ((patches.to(device) - dictionary @ coefficients).t(), coefficients.t()),\n",
    "        ).mean(dim=0)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def display_dictionary(dictionary: torch.Tensor) -> None:\n",
    "    \"\"\"Display a dictionary of basis vectors.\"\"\"\n",
    "    patch_size = int(math.sqrt(dictionary.shape[0]))\n",
    "    images = [\n",
    "        dictionary[:, _].clone().reshape(patch_size, -1)\n",
    "        for _ in range(dictionary.shape[-1])\n",
    "    ]\n",
    "    display_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e897d6b3",
   "metadata": {},
   "source": [
    "## Q1: Sparse coding of natural images (10 points)\n",
    "\n",
    "In this section, you will learn sparse encodings of natural images using the images from the original [Olshausen & Field (1996) study](https://www.nature.com/articles/381607a0).\n",
    "\n",
    "- Plot the basis functions learnt from these images for each of these dictionary sizes: 25, 64, 100. (3 points)\n",
    "- Describe the basis functions learnt. (3 points)\n",
    "- Why do they look this way? (4 points)\n",
    "\n",
    "Feel free to alter the default parameters! If any parameter combination gives significantly better results than the defaults (or speeds up the convergence), please report them in your answer so that we can update the defaults for future semesters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images(download_images())\n",
    "display_images(images)\n",
    "\n",
    "dictionary = learn_sparse_encoding(\n",
    "    datapipe=extract_patches_from_images(images=images),\n",
    "    dictionary_size=64,\n",
    "    n_batches=2000,\n",
    ")\n",
    "\n",
    "display_dictionary(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873f2ba",
   "metadata": {},
   "source": [
    "## Q2: Sparse coding of sinusoids (10 points)\n",
    "\n",
    "In this section, you will learn sparse encodings for sinusoids of different frequencies.\n",
    "\n",
    "- Learn dictionaries of basis functions for sinsuoids of (i) low-frequency (e.g. 0.5) and (ii) high-frequency (e.g. 3) using different dictionary sizes (4, 36, and 64). Plot these basis functions. (3 points)\n",
    "- What variations do you observe when changing frequency and dictionary size? (3 points)\n",
    "- Why do these variations arise? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinusoids = create_sinusoids(frequency=3)\n",
    "display_images(sinusoids)\n",
    "\n",
    "dictionary = learn_sparse_encoding(\n",
    "    datapipe=extract_patches_from_images(images=sinusoids),\n",
    "    dictionary_size=64,\n",
    "    n_batches=100,\n",
    ")\n",
    "\n",
    "display_dictionary(dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('neural-dimensionality')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8aca399878df662d12a33c13450d8b5abb2859ca28936f41d8414d834a053a1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
